#########################################
### This is the default configuration for gearpump
### To use the application, you at least need to change gearpump.cluster to point to right master
#########################################
gearpump {

  ###########################
  ### Change the dispather for tasks
  ### If you don't know what this is about, don't change it
  ###########################

  task-dispatcher = "akka.actor.pined-dispatcher"
  netty-dispatcher = "akka.actor.default-dispatcher"

  ###########################
  ### Metrics setting,
  ### If you want to use metrics, please change
  ###########################


  ### Flag to enable metrics
  metrics {
    enabled = false

    # We will take one metric out of ${sample.rate}
    sample-rate = 10

    report-interval-ms = 15000

    # reporter = "logfile"
    reporter = "graphite"

    graphite {
      ## Graphite host settings
      host = "127.0.0.1"
      port = 2003
    }

    logfile {

    }
  }

  #######################################
  ### Logging settings
  #######################################
  # The log dir for daemon processes
  log.daemon.dir = "logs"

  # The log dir for applications
  log.application.dir = "logs"

  serializers {
    ## Follow this format when adding new serializer for new message types
    ##    "org.apache.gearpump.Message" = "org.apache.gearpump.streaming.MessageSerializer"
    "org.apache.gearpump.Message" = "org.apache.gearpump.streaming.MessageSerializer"
    "org.apache.gearpump.streaming.task.AckRequest" = "org.apache.gearpump.streaming.AckRequestSerializer"
    "org.apache.gearpump.streaming.task.Ack" = "org.apache.gearpump.streaming.AckSerializer"
    "org.apache.gearpump.streaming.task.TaskId" = "org.apache.gearpump.streaming.TaskIdSerializer"

    ## Use default serializer for these types
    "scala.collection.immutable.List" = ""
    "scala.collection.immutable.$colon$colon" = ""
    "[B" = ""
    "[C" = ""
    "[S" = ""
    "[I" = ""
    "[J" = ""
    "[F" = ""
    "[D" = ""
    "[Z" = ""
    "[Ljava.lang.String;" = ""
    "scala.Tuple1" = ""
    "scala.Tuple2" = ""
    "scala.Tuple3" = ""
    "scala.Tuple4" = ""
    "scala.Tuple5" = ""
    "scala.Tuple6" = ""


  }

  ## How many slots each worker contains
  worker.slots = 100

  ###################
  ### Appmaster JVM argument configuration
  ###################
  appmaster {
    vmargs = "-server -Xms128M -Xmx256M -Xss1M -XX:MaxPermSize=128m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseParNewGC -XX:NewRatio=3"
    extraClasspath = ""
  }

  ###################
  ### Executor argument configuration
  ### Executor JVM can contains multiple tasks
  ###################
  executor {
    vmargs = "-server -Xms128M -Xmx256M -Xss1M -XX:MaxPermSize=128m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseParNewGC -XX:NewRatio=3"
    extraClasspath = ""
  }

  ##############################
  ### Required to change!!
  ### You need to set the master cluster address here
  ###
  ###
  ### For example, you may start three master
  ### on node1: bin/master -ip node1 -port 3000
  ### on node2: bin/master -ip node2 -port 3000
  ### on node3: bin/master -ip node3 -port 3000
  ###
  ### Then you need to set the cluster.masters = ["node1:3000","node2:3000","node3:3000"]
  cluster {
    masters = []
  }

  ### Define where the submitted jar file will be stored at

  ### This path follows the hadoop path schema
  ### For HDFS, use hdfs://host:port/path/
  ### For FTP, use ftp://host:port/path
  ### If you want to store on master nodes, then use local directory,
  ### jarstore.rootpath = "jarstore/" will points to relative directory where master is started.
  ### jarstore.rootpath = "/jarstore/" will points to absolute directory on master server
  jarstore.rootpath = "jarstore/"

  #########################
  ### Scheduller for master, it will use this scheduler to schedule resource for
  ### different applications.
  ### If you don't know what is this about, don't change it
  #########################
  scheduling {
    scheduler-class = "org.apache.gearpump.cluster.scheduler.PriorityScheduler"
  }

  #############################################
  # Default Configuration for REST service
  #############################################

  #########################
  ### REST service can be started by a command line tool bin/rest
  ### If you don't know what is this about, don't change it
  #########################
  rest-services {
    host = "127.0.0.1"
    port = 8090
  }

  #############################################
  ## Default Configuration for Gearpump Netty transport layer
  ## If you don't know what is this about, don't change it
  #############################################
  netty {
    buffer-size = 5242880
    max-retries = 30
    base-sleep-ms = 100
    max-sleep-ms = 1000
    message-batch-size = 262144
    fulsh-check-interval = 10
  }
}

### Akka system configuration for master nodes
master {
  extensions = [
    "akka.contrib.datareplication.DataReplication$"
  ]
  akka {
    loglevel = "INFO"
    log-dead-letters = off
    log-dead-letters-during-shutdown = off
    actor {
      ## Master forms a akka cluster
      provider = "akka.cluster.ClusterActorRefProvider"
    }
    cluster {
      roles = ["master"]
      auto-down-unreachable-after = 15s
    }
    remote {
      log-remote-lifecycle-events = off
    }
  }
}

### Akka system configuration for worker nodes
worker {
  ## Add worker overrided config
  akka {
    loglevel = "INFO"
    log-dead-letters = off
    log-dead-letters-during-shutdown = off
    actor {
      provider = "akka.remote.RemoteActorRefProvider"
    }
    cluster {
      roles = ["worker"]
    }
    remote {
      log-remote-lifecycle-events = off
    }
  }
}

### Akka system configuration for master nodes, worker nodes, and clients
base {
  spray.can {
    ### allow user to post jar less than 200MB
    server.parsing.max-content-length = "1000M"
  }

  akka {
    extensions = [
      "com.romix.akka.serialization.kryo.KryoSerializationExtension$",
      "org.apache.gearpump.transport.Express$",
      "org.apache.gearpump.metrics.Metrics$"
    ]
    loglevel = "INFO"
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    log-dead-letters = off
    log-dead-letters-during-shutdown = off
    actor {
      provider = "akka.remote.RemoteActorRefProvider"
      default-mailbox {
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
      }
      default-dispatcher {
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
        throughput = 10
        fork-join-executor {
          parallelism-factor = 2
          parallelism-max = 4
          parallelism-min = 4
        }
      }
      pined-dispatcher {
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
        executor = "thread-pool-executor"
        type = "PinnedDispatcher"
      }
      serializers {
        kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
      }
      serialization-bindings {
      }

      kryo {
        buffer-size = 4096
        classes = [
        ]
        kryo-custom-serializer-init = "org.apache.gearpump.serializer.GearpumpSerialization"
        compression = off
        idstrategy = "incremental"
        implicit-registration-logging = true
        kryo-reference-map = true
        kryo-trace = false
        mappings {
        }
        max-buffer-size = -1
        serializer-pool-size = 16
        type = "graph"
        use-manifests = false
      }
    }
    remote {
      log-remote-lifecycle-events = off
      use-dispatcher = "akka.remote.default-remote-dispatcher"
      enabled-transports = ["akka.remote.netty.tcp"]
      netty.tcp {
        port = 0
        server-socket-worker-pool {
          pool-size-min = 1
          pool-size-max = 1
        }
        client-socket-worker-pool {
          pool-size-min = 1
          pool-size-max = 1
        }
      }
      default-remote-dispatcher {
        throughput = 5
        type = Dispatcher
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
        executor = "fork-join-executor"
        fork-join-executor {
          parallelism-min = 1
          parallelism-max = 1
        }
      }
    }
  }
}
